El proyecto permitió construir una canalización completa para extraer, procesar y analizar datos temporales provenientes de una base de datos SQLite. A partir de los registros de tiempo, fue posible calcular los delays entre eventos y estudiar su comportamiento estadístico mediante técnicas de análisis exploratorio, visualización y ajuste probabilístico.

Los resultados mostraron que los datos presentan una distribución claramente asimétrica y con colas largas, características típicas de procesos aleatorios de llegada. Asimismo, se determinó que la distribución que mejor se ajusta al comportamiento de los tiempos entre mensajes es un modelo exponencial, lo cual sugiere que las llegadas de eventos son razonablemente independientes y siguen un proceso estocástico cercano a un Proceso de Poisson.

La comparación de los momentos estadísticos (media, varianza, asimetría y curtosis) entre los datos reales y el modelo ajustado mostró diferencias pequeñas, lo que valida que el modelo teórico representa adecuadamente el comportamiento observado. Además, la generación de histogramas —tanto con ajuste como sin ajuste— permitió visualizar de forma clara la relación entre los datos reales y la distribución estimada.

En conjunto, el proyecto demuestra que es posible automatizar un flujo completo de análisis estadístico: desde la extracción de datos, su transformación y limpieza, hasta la selección del modelo probabilístico que mejor describe el fenómeno. Esto no solo agiliza el procesamiento de información, sino que también proporciona una base estadística sólida para interpretaciones posteriores, predicciones o la futura implementación de sistemas basados en probabilidad.